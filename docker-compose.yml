version: "3.7"
services:


##############################################################################################################
    minio:
        container_name: minio
        build:
            context: ./dockerfiles/dockerfile_minio
        restart: always
        working_dir: "/minio-image/storage"
        volumes:
            - ./Storage/minio/storage:/minio-image/storage
        ports:
            - "9000:9000"
        environment:
            MINIO_ACCESS_KEY: minio-image
            MINIO_SECRET_KEY: minio-image-pass
        command: server /minio-image/storage


##############################################################################################################
    mlflow:
        container_name: "mlflow"
        build:
            context: ./dockerfiles/dockerfile_mlflowserver
        working_dir: "/mlflow-image"
        volumes:
            - ./Storage/mlflow:/mlflow-image
        environment:
            MLFLOW_S3_ENDPOINT_URL: http://minio-image:9000
            AWS_ACCESS_KEY_ID: minio-image
            AWS_SECRET_ACCESS_KEY: minio-image-pass
        ports:
            - "5500:5500"
        command: mlflow server --host 0.0.0.0 --port 5500 --backend-store-uri /mlflow-image/mlruns 


##############################################################################################################
    jupyter:
        container_name: "jupyter"
        build:
            context: ./dockerfiles/dockerfile_jupyter_notebook
        volumes:
            - ./Storage/notebooks:/home/jovyan/work
            - ./Storage/q_pack:/home/jovyan/work/q_pack
            - ./Storage/mlflow:/mlflow-image
        environment:
            MLFLOW_S3_ENDPOINT_URL: http://minio-image:9000
            AWS_ACCESS_KEY_ID: minio-image
            AWS_SECRET_ACCESS_KEY: minio-image-pass    
            MLFLOW_TRACKING_URI: http://mlflow-image:5500        
        ports:    
            - "8888:8888"


##############################################################################################################
    etl:
        container_name: "etl"
        build:
            context: ./dockerfiles/dockerfile_etl
        volumes:
            - ./Storage/etl:/app
            - ./Storage/q_pack:/app/q_pack
        environment:
            MLFLOW_S3_ENDPOINT_URL: http://minio-image:9000
            AWS_ACCESS_KEY_ID: minio-image
            AWS_SECRET_ACCESS_KEY: minio-image-pass    
            MLFLOW_TRACKING_URI: http://mlflow-image:5500 
            # PORT: '66'       
        ports:    
            - "8060:8060"
        command: gunicorn -c gunicorn_config.py -b :8060 app:app

            
##############################################################################################################
    redis:
        container_name: redis
        image: redis
        restart: always
        volumes:
            - redis:/data
        ports:
            - 6379:6379


##############################################################################################################
    postgres_secmaster: 
        image: postgres
        restart: always
        container_name: "my_postgresql"
        ports:
            - 5431:5431                
        environment:
            - SHARED_PASSWORD=password
            - POSTGRES_PASSWORD=posgres349
        volumes:
            - ./Storage/postgress_db/scripts/:/docker-entrypoint-initdb.d/ 
            - pg_data:/var/lib/postgresql/data


##############################################################################################################
    pgadmin:
        image : dpage/pgadmin4
        container_name: pgadmin
        user: "${UID}:${GID}"
        environment:
            PGADMIN_DEFAULT_EMAIL: "admin@admin.org"
            PGADMIN_DEFAULT_PASSWORD: "admin"
            PGADMIN_LISTEN_PORT: 1234
        volumes:
            - ./Storage/pgadmin/:/var/lib/pgadmin 
        ports:
            - 1234:1234
        depends_on:
            - postgres_secmaster


##############################################################################################################
    postgresql:
#        image: postgres
        image: bitnami/postgresql:latest
        container_name: "airflow-postgresql"
        environment:
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
            # ALLOW_EMPTY_PASSWORD is recommended only for development.
            - ALLOW_EMPTY_PASSWORD=yes
        volumes:
            - pg_data_airflow:/var/lib/postgresql/data



##############################################################################################################
    airflow-scheduler:
        image: bitnami/airflow:latest
        container_name: "airflow-scheduler"
        restart: always
        environment:
            - AIRFLOW_COMPONENT_TYPE=scheduler
            - AIRFLOW_DATABASE_NAME=airflow
            - AIRFLOW_DATABASE_USERNAME=airflow
            - AIRFLOW_DATABASE_PASSWORD=airflow
            - AIRFLOW_EXECUTOR=CeleryExecutor
            - AIRFLOW_APISERVER_HOST=airflow
            - AIRFLOW_FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
            - AIRFLOW_SECRET_KEY=a25mQ1FHTUh3MnFRSk5KMEIyVVU2YmN0VGRyYTVXY08=
            - AIRFLOW_APISERVER_HOST=airflow
            - AIRFLOW_LOAD_EXAMPLES=yes
        depends_on:
            - postgresql
            - redis
            - airflow
        volumes:
            - ./Storage/airflow/dags:/usr/local/airflow/dags
            - ./Storage/q_pack:/usr/local/airflow/dags/q_pack


##############################################################################################################
    airflow-triggerer:
        image: bitnami/airflow:latest
        container_name: "airflow-triggerer"
        restart: always
        environment:
            - AIRFLOW_COMPONENT_TYPE=triggerer
            - AIRFLOW_DATABASE_NAME=airflow
            - AIRFLOW_DATABASE_USERNAME=airflow
            - AIRFLOW_DATABASE_PASSWORD=airflow
            - AIRFLOW_EXECUTOR=CeleryExecutor
            - AIRFLOW_APISERVER_HOST=airflow
            - AIRFLOW_FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
            - AIRFLOW_SECRET_KEY=a25mQ1FHTUh3MnFRSk5KMEIyVVU2YmN0VGRyYTVXY08=
            - AIRFLOW_APISERVER_HOST=airflow
            - AIRFLOW_LOAD_EXAMPLES=yes
        depends_on:
            - postgresql
            - redis
            - airflow
        volumes:
            - ./Storage/airflow/dags:/usr/local/airflow/dags
            - ./Storage/q_pack:/usr/local/airflow/dags/q_pack


##############################################################################################################
    airflow-dag-processor:
        image: bitnami/airflow:latest
        container_name: "airflow-dag-processor"
        restart: always
        environment:
            - AIRFLOW_COMPONENT_TYPE=dag-processor
            - AIRFLOW_DATABASE_NAME=airflow
            - AIRFLOW_DATABASE_USERNAME=airflow
            - AIRFLOW_DATABASE_PASSWORD=airflow
            - AIRFLOW_EXECUTOR=CeleryExecutor
            - AIRFLOW_APISERVER_HOST=airflow
            - AIRFLOW_FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
            - AIRFLOW_SECRET_KEY=a25mQ1FHTUh3MnFRSk5KMEIyVVU2YmN0VGRyYTVXY08=
            - AIRFLOW_APISERVER_HOST=airflow
            - AIRFLOW_LOAD_EXAMPLES=yes
        depends_on:
            - postgresql
            - redis
            - airflow
        volumes:
            - ./Storage/airflow/dags:/usr/local/airflow/dags
            - ./Storage/q_pack:/usr/local/airflow/dags/q_pack


##############################################################################################################
    airflow-worker:
        image: bitnami/airflow:latest
        container_name: "airflow-worker"
        restart: always
        environment:
            - AIRFLOW_COMPONENT_TYPE=worker
            - AIRFLOW_DATABASE_NAME=airflow
            - AIRFLOW_DATABASE_USERNAME=airflow
            - AIRFLOW_DATABASE_PASSWORD=airflow
            - AIRFLOW_EXECUTOR=CeleryExecutor
            - AIRFLOW_APISERVER_HOST=airflow
            - AIRFLOW_FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
            - AIRFLOW_SECRET_KEY=a25mQ1FHTUh3MnFRSk5KMEIyVVU2YmN0VGRyYTVXY08=
            - AIRFLOW_APISERVER_HOST=airflow
            - AIRFLOW_LOAD_EXAMPLES=yes
        depends_on:
            - postgresql
            - redis
            - airflow
        volumes:
            - ./Storage/airflow/dags:/usr/local/airflow/dags
            - ./Storage/q_pack:/usr/local/airflow/dags/q_pack


##############################################################################################################
    airflow:
        image: bitnami/airflow:latest
        container_name: "airflow"
        build:
            context: ./dockerfiles/dockerfile_dash
        restart: always
        environment:
            - AIRFLOW_DATABASE_NAME=airflow
            - AIRFLOW_DATABASE_USERNAME=airflow
            - AIRFLOW_DATABASE_PASSWORD=airflow
            - AIRFLOW_EXECUTOR=CeleryExecutor
            - AIRFLOW_FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
            - AIRFLOW_SECRET_KEY=a25mQ1FHTUh3MnFRSk5KMEIyVVU2YmN0VGRyYTVXY08=
            - AIRFLOW_APISERVER_HOST=airflow
            - AIRFLOW_PASSWORD=user
            - AIRFLOW_USERNAME=user
            - AIRFLOW_EMAIL=user@example.com
            - AIRFLOW__WEBSERVER__AUTH_BACKEND=airflow.contrib.auth.backends.password_auth
        ports:
            - '8080:8080'
        depends_on:
            - postgresql
            - redis
        volumes:
            - ./Storage/airflow/dags:/usr/local/airflow/dags
            - ./Storage/q_pack:/usr/local/airflow/dags/q_pack
        healthcheck:
              test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
              interval: 30s
              timeout: 10s
              retries: 5
              start_period: 30s



##############################################################################################################
#    airflow:
#        #image: apache/airflow:latest
#        image: bitnami/airflow:latest
#        container_name: "airflow"
#        build:
#            context: ./dockerfiles/dockerfile_airflow
#        restart: always
#        depends_on:
#            - postgres
#            - redis
#        environment:
#            - LOAD_EX=n
#            - FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
#            - EXECUTOR=Celery
#            # - EXECUTOR=Local
#            - AIRFLOW__WEBSERVER__AUTHENTICATE=True
#            - AIRFLOW__WEBSERVER__AUTH_BACKEND=airflow.contrib.auth.backends.password_auth
#        volumes:
#            - ./Storage/airflow/dags:/usr/local/airflow/dags
#            - ./Storage/q_pack:/usr/local/airflow/dags/q_pack
#        ports:
#            - 8080:8080
        #command: webserver
        #command: api-server
#        healthcheck:
#              test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
#              interval: 30s
#              timeout: 10s
#              retries: 5
#              start_period: 30s



##############################################################################################################
#    flower:
#        #image: apache/airflow:latest
#        image: bitnami/airflow:latest
#        restart: always
#        depends_on:
#            - redis
#        environment:
#            - EXECUTOR=Celery
#            - AIRFLOW__WEBSERVER__AUTHENTICATE=True
#            - AIRFLOW__WEBSERVER__AUTH_BACKEND=airflow.contrib.auth.backends.password_auth
#        ports:
#            - "5555:5555"
#        command: flower --basic_auth=user1:password1


##############################################################################################################
#    scheduler:
#        #image: apache/airflow:latest
#        image: bitnami/airflow:latest
#        restart: always
#        depends_on:
#            - airflow
#        volumes:
#            - ./Storage/airflow/dags:/usr/local/airflow/dags
#            - ./Storage/q_pack:/usr/local/airflow/dags/q_pack
#        environment:
#            - LOAD_EX=n
#            - FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
#            - EXECUTOR=Celery
#        command: scheduler


##############################################################################################################
#    worker:
#        #image: apache/airflow:latest
#        image: bitnami/airflow:latest
#        restart: always
#        depends_on:
#            - scheduler
#        volumes:
#            - ./Storage/airflow/dags:/usr/local/airflow/dags
#            - ./Storage/q_pack:/usr/local/airflow/dags/q_pack

#        environment:
#            - FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
#            - EXECUTOR=Celery
#            - C_FORCE_ROOT=true
#        command: worker


##############################################################################################################
    dash:
        container_name: "dash"
        build:
            context: ./dockerfiles/dockerfile_dash
        restart: always
        ports:
            - 8050:8050
            - "5001:5001" # app(debug) port
            - "3000:3000" # remote debugger attach port
        depends_on:
            - postgresql
        volumes:
            - ./Storage/dash:/app
            - ./Storage/q_pack:/q_pack
        environment: 
            - FLASK_DEBUG=1
            - FLASK_APP=app.py
            - FLASK_ENV=development


##############################################################################################################
    twportainer:
        image: portainer/portainer-ce:latest
        container_name: twportainer
        environment:
            - TZ=Europe/Moscow
        volumes:
            - /var/run/docker.sock:/var/run/docker.sock
            - ./Storage/twportainer/portainer_data:/data
        ports:
            - "8000:8000"
            - "9443:9443"
        restart: always


##############################################################################################################
    nginx:
        image: nginx
        container_name: "nginx_reverse"
        volumes:
            - ./Storage/nginx/:/etc/nginx/
        ports:
            - 80:80
        depends_on:
            - "jupyter"
            - "airflow"
            - "mlflow"
            - "pgadmin"
            - "minio"
            - "twportainer"
        links:
            - "jupyter"
            - "airflow"
            - "mlflow"
            - "pgadmin"
            - "minio"
            - "twportainer"
        restart: always



##############################################################################################################
volumes:
    pg_data:
        external: false
        name: pg_data
    pg_data_airflow:
        external: false
        name: pg_data_airflow
    redis:
        external: false
        name: redis


# docker-compose up -d --scale worker=5